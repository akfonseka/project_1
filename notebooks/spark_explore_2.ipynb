{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F \n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.context import SparkContext\n",
    "from config_variables import var_credentials_location, var_gcs_connector\n",
    "import requests\n",
    "import pandas as pd\n",
    "from io import StringIO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for Spark Cluster\n",
    "conf = SparkConf() \\\n",
    "    .setMaster('local[*]') \\\n",
    "    .setAppName('GCS Ingestion') \\\n",
    "    .set(\"spark.jars\", var_gcs_connector) \\\n",
    "    .set(\"spark.hadoop.google.cloud.auth.service.account.enable\", \"true\") \\\n",
    "    .set(\"spark.hadoop.google.cloud.auth.service.account.json.keyfile\", var_credentials_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/09 09:47:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "#  Configuring Hadoop\n",
    "\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "hadoop_conf = sc._jsc.hadoopConfiguration()\n",
    "\n",
    "hadoop_conf.set(\"fs.AbstractFileSystem.gs.impl\",  \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\n",
    "hadoop_conf.set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
    "hadoop_conf.set(\"fs.gs.auth.service.account.json.keyfile\", var_credentials_location)\n",
    "hadoop_conf.set(\"fs.gs.auth.service.account.enable\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .config(conf=sc.getConf()) \\\n",
    "    .getOrCreate() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading GCS Data into Spark DF\n",
    "df_15 = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv(\"gs://pipelineproject01-data-bronze-bucket/raw/2015/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Year: string (nullable = true)\n",
      " |-- UnqID: string (nullable = true)\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Weather: string (nullable = true)\n",
      " |-- Time: string (nullable = true)\n",
      " |-- Day: string (nullable = true)\n",
      " |-- Round: string (nullable = true)\n",
      " |-- Dir: string (nullable = true)\n",
      " |-- Path: string (nullable = true)\n",
      " |-- Mode: string (nullable = true)\n",
      " |-- Count: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_15.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+----------+-------+--------+-------+-----+----------+----+--------------+-----+\n",
      "|Year|   UnqID|      Date|Weather|    Time|    Day|Round|       Dir|Path|          Mode|Count|\n",
      "+----+--------+----------+-------+--------+-------+-----+----------+----+--------------+-----+\n",
      "|2015|CSHCY005|10/10/2014|    Wet|06:00:00|Weekday|    A|Northbound|null|Private cycles|    0|\n",
      "|2015|CSHCY005|10/10/2014|    Wet|06:15:00|Weekday|    A|Northbound|null|Private cycles|    0|\n",
      "|2015|CSHCY005|10/10/2014|    Wet|06:30:00|Weekday|    A|Northbound|null|Private cycles|    0|\n",
      "|2015|CSHCY005|10/10/2014|    Dry|06:45:00|Weekday|    A|Northbound|null|Private cycles|    0|\n",
      "|2015|CSHCY005|10/10/2014|    Dry|07:00:00|Weekday|    A|Northbound|null|Private cycles|    0|\n",
      "|2015|CSHCY005|10/10/2014|    Dry|07:15:00|Weekday|    A|Northbound|null|Private cycles|    2|\n",
      "|2015|CSHCY005|10/10/2014|    Dry|07:30:00|Weekday|    A|Northbound|null|Private cycles|    0|\n",
      "|2015|CSHCY005|10/10/2014|    Dry|07:45:00|Weekday|    A|Northbound|null|Private cycles|    0|\n",
      "|2015|CSHCY005|10/10/2014|    Dry|08:00:00|Weekday|    A|Northbound|null|Private cycles|    2|\n",
      "|2015|CSHCY005|10/10/2014|    Dry|08:15:00|Weekday|    A|Northbound|null|Private cycles|    2|\n",
      "|2015|CSHCY005|10/10/2014|    Dry|08:30:00|Weekday|    A|Northbound|null|Private cycles|    2|\n",
      "|2015|CSHCY005|10/10/2014|    Dry|08:45:00|Weekday|    A|Northbound|null|Private cycles|    0|\n",
      "|2015|CSHCY005|10/10/2014|    Dry|09:00:00|Weekday|    A|Northbound|null|Private cycles|    0|\n",
      "|2015|CSHCY005|10/10/2014|    Dry|09:15:00|Weekday|    A|Northbound|null|Private cycles|    3|\n",
      "|2015|CSHCY005|10/10/2014|    Dry|09:30:00|Weekday|    A|Northbound|null|Private cycles|    0|\n",
      "|2015|CSHCY005|10/10/2014|    Dry|09:45:00|Weekday|    A|Northbound|null|Private cycles|    2|\n",
      "|2015|CSHCY005|09/10/2014|    Wet|10:00:00|Weekday|    A|Northbound|null|Private cycles|    2|\n",
      "|2015|CSHCY005|09/10/2014|    Wet|10:15:00|Weekday|    A|Northbound|null|Private cycles|    0|\n",
      "|2015|CSHCY005|09/10/2014|    Wet|10:30:00|Weekday|    A|Northbound|null|Private cycles|    2|\n",
      "|2015|CSHCY005|09/10/2014|    Wet|10:45:00|Weekday|    A|Northbound|null|Private cycles|    0|\n",
      "+----+--------+----------+-------+--------+-------+-----+----------+----+--------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_15.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataSet Info\n",
    "\n",
    "UnqID - UUID for London locations\n",
    "\n",
    "Count - Disaggregated by site, direction, 15-min period and by path and mode classifications that vary depending on the area and year\n",
    "\n",
    "Mode - Drop this column\n",
    "\n",
    "Path - Drop this column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ariths-macbook:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>GCS Ingestion</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7ff02a8eeec0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
