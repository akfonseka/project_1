{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F \n",
    "from pyspark.sql import types\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.context import SparkContext\n",
    "from config_variables import var_credentials_location, var_gcs_connector\n",
    "import requests\n",
    "import pandas as pd\n",
    "from io import StringIO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for Spark Cluster\n",
    "conf = SparkConf() \\\n",
    "    .setMaster('local[*]') \\\n",
    "    .setAppName('Data Cleaning + Loading to BQ') \\\n",
    "    .set(\"spark.jars\", var_gcs_connector) \\\n",
    "    .set(\"spark.hadoop.google.cloud.auth.service.account.enable\", \"true\") \\\n",
    "    .set(\"spark.hadoop.google.cloud.auth.service.account.json.keyfile\", var_credentials_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/09 11:20:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "#  Configuring Hadoop\n",
    "\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "hadoop_conf = sc._jsc.hadoopConfiguration()\n",
    "\n",
    "hadoop_conf.set(\"fs.AbstractFileSystem.gs.impl\",  \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\n",
    "hadoop_conf.set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
    "hadoop_conf.set(\"fs.gs.auth.service.account.json.keyfile\", var_credentials_location)\n",
    "hadoop_conf.set(\"fs.gs.auth.service.account.enable\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .config(conf=sc.getConf()) \\\n",
    "    .getOrCreate() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_schema = types.StructType([\n",
    "    types.StructField('Year', types.IntegerType(), True), \n",
    "    types.StructField('UnqID', types.StringType(), True), \n",
    "    types.StructField('Date', types.StringType(), True), \n",
    "    types.StructField('Weather', types.StringType(), True), \n",
    "    types.StructField('Time', types.TimestampType(), True), \n",
    "    types.StructField('Day', types.StringType(), True), \n",
    "    types.StructField('Round', types.StringType(), True), \n",
    "    types.StructField('Dir', types.StringType(), True), \n",
    "    types.StructField('Path', types.StringType(), True), \n",
    "    types.StructField('Mode', types.StringType(), True), \n",
    "    types.StructField('Count', types.IntegerType(), True)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading GCS Data into Spark DF\n",
    "df_test = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(bike_schema) \\\n",
    "    .csv(\"gs://pipelineproject01-data-bronze-bucket/raw/2015/Central/*\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_columns = [\"Date\", \"UUID\", \"Weather\", \"Time\", \"Day\", \"Area\", \"Count\"]\n",
    "\n",
    "df_test = df_test \\\n",
    "        .withColumn(\"Date\", F.to_date(df_test[\"Date\"], \"dd/MM/yyyy\")) \\\n",
    "        .withColumn(\"Time\", F.date_format(df_test[\"Time\"], \"HH:mm:ss\")) \\\n",
    "        .withColumnRenamed(\"UnqID\", \"UUID\") \\\n",
    "        .withColumn(\"Area\", F.lit(\"Central\")) \\\n",
    "        .select(final_columns) \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.createOrReplaceTempView(\"test_data\")\n",
    "\n",
    "df_test2 = spark.sql(\"\"\"\n",
    "select \n",
    "        UUID\n",
    "        ,Area\n",
    "        ,Date\n",
    "        ,Time\n",
    "        ,Day\n",
    "        ,CASE\n",
    "            WHEN Weather IN \n",
    "                ('Dry', 'Dry/hot', 'Dry Warm', 'Dry & Windy', 'Dry Chill', 'Dry/cold', 'Sunny Dry'\n",
    "                , 'Sun', 'Bright', 'Sunny', 'Sunny Overcast', 'Sunny/cloudy', 'Partly Sunny', 'Bright + Cloudy'\n",
    "                ,'Sun Setting' ,'Sunny + Cloudy' ,'Fine/dry', 'Good', 'Fine', \"Sun\", \"Dry              ...\"\n",
    "                , \"Dry/ Sunny\", \"Cloudy/sunny\", \"Warm + Dry\", \"Druy\", \"Dry\", \"Dry/Sunny\", \"Dry/sun\", \"Fine + Dry\"\n",
    "                , \"Dry/good\", \"Dry/mild\", \"Fine + Hot\", \"Fair\", \"Sunny Dry\", \"Dry/sunny\", \" Dry\") \n",
    "            THEN 'Clear/Sunny'\n",
    "            WHEN Weather IN \n",
    "                ('Cloudy', 'Cloudy/dry', 'Overcast', 'Cloudy Sunny' , 'Cloudy + Sunny', 'Cold/cloudy'\n",
    "                ,'Dry/overcast', \"Dark/cloudy\", \"Dark Cloudy\", \"Dull\") \n",
    "            THEN 'Cloudy'\n",
    "            WHEN Weather IN \n",
    "                ('Wet/damp', 'Wet ', 'Very Wet', 'Damp', 'Drizzle', 'Light Shrs', 'Mizzle'\n",
    "                ,'V. Light Drizzle', 'Light Rain', 'Drizzly]', 'Drtizzly', 'Drizzel', 'Drizzly'\n",
    "                ,'Light Showers', 'Dry Wet Road' ,'Mix Wet/dry', 'V. Light Drizzle', 'Dry Wet Road'\n",
    "                , \"S.wet\", \"S/w\", \"Wet\", \"Wet/dry\", \"Showers\", \"S. Wet\", \"Shower\", \"Some Showers\"\n",
    "                , \"Road Wet\", \"Dry/wet\") \n",
    "            THEN 'Light Rain'\n",
    "            WHEN Weather IN \n",
    "                ('Windy/rain', 'Windy + Sunny', 'Wind/ Showers', 'Windy + Sunny', 'Windy', 'Windy/rain'\n",
    "                ,'Rain & Cloudy', 'Windy/wet', 'Rain & Windy', 'Windy/rain', 'Very Windy', 'Cloudy + Rain'\n",
    "                ,'Cloudy/rain/sunny' , 'Sunsetting + Windy', 'High Wind' ,'Cold', 'Cold/cloudy'\n",
    "                , \"Cloudy/rain\", \"Rain\", \"Wet And Windy\", \"Dry/windy\", \"Dry Very Windy\", \"Raining\"\n",
    "                , \"Wet/windy\", \"Dry Cold\", \"Rain/cloudy\", \"Heavy Rain\", \"Cold/sunny\", \"Cloudy/windy\"\n",
    "                , \"Wet/v.windy\", \"Rains\", \"Dark/dry\", \"Dry/dark\", \"Dark Dry\", \"Dry Dark\") \n",
    "            THEN 'Windy/Rainy/Cold'\n",
    "            ELSE \"Unknown\"\n",
    "        END AS GroupedWeather \n",
    "        ,Count\n",
    "from \n",
    "        test_data \n",
    ";\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+----------+--------+-------+--------------+-----+\n",
      "|    UUID|   Area|      Date|    Time|    Day|GroupedWeather|Count|\n",
      "+--------+-------+----------+--------+-------+--------------+-----+\n",
      "|CENCY001|Central|2015-06-04|06:00:00|Weekday|   Clear/Sunny|    3|\n",
      "|CENCY001|Central|2015-06-04|06:15:00|Weekday|   Clear/Sunny|   10|\n",
      "|CENCY001|Central|2015-06-04|06:30:00|Weekday|   Clear/Sunny|   18|\n",
      "|CENCY001|Central|2015-06-04|06:45:00|Weekday|   Clear/Sunny|   39|\n",
      "|CENCY001|Central|2015-06-04|07:00:00|Weekday|   Clear/Sunny|   65|\n",
      "|CENCY001|Central|2015-06-04|07:15:00|Weekday|   Clear/Sunny|   83|\n",
      "|CENCY001|Central|2015-06-04|07:30:00|Weekday|   Clear/Sunny|  118|\n",
      "|CENCY001|Central|2015-06-04|07:45:00|Weekday|   Clear/Sunny|  153|\n",
      "|CENCY001|Central|2015-06-04|08:00:00|Weekday|   Clear/Sunny|  165|\n",
      "|CENCY001|Central|2015-06-04|08:15:00|Weekday|   Clear/Sunny|  184|\n",
      "|CENCY001|Central|2015-06-04|08:30:00|Weekday|   Clear/Sunny|  215|\n",
      "|CENCY001|Central|2015-06-04|08:45:00|Weekday|   Clear/Sunny|  172|\n",
      "|CENCY001|Central|2015-06-04|09:00:00|Weekday|   Clear/Sunny|  112|\n",
      "|CENCY001|Central|2015-06-04|09:15:00|Weekday|   Clear/Sunny|   78|\n",
      "|CENCY001|Central|2015-06-04|09:30:00|Weekday|   Clear/Sunny|   52|\n",
      "|CENCY001|Central|2015-06-04|09:45:00|Weekday|   Clear/Sunny|   31|\n",
      "|CENCY001|Central|2015-06-05|10:00:00|Weekday|   Clear/Sunny|   25|\n",
      "|CENCY001|Central|2015-06-05|10:15:00|Weekday|   Clear/Sunny|   19|\n",
      "|CENCY001|Central|2015-06-05|10:30:00|Weekday|   Clear/Sunny|   22|\n",
      "|CENCY001|Central|2015-06-05|10:45:00|Weekday|   Clear/Sunny|   15|\n",
      "+--------+-------+----------+--------+-------+--------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_test2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataSet Info\n",
    "\n",
    "UnqID - UUID for London locations\n",
    "\n",
    "Count - Disaggregated by site, direction, 15-min period and by path and mode classifications that vary depending on the area and year\n",
    "\n",
    "Mode - Drop this column\n",
    "\n",
    "Path - Drop this column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ariths-macbook:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>GCS Ingestion</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7ff02a8eeec0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
