# Base Python image
FROM python:3.10.1

# Install necessary dependencies 
RUN apt-get update && \
    apt-get install -y openjdk-11-jre-headless && \
    apt-get install -y curl

# Working directory 
WORKDIR /app

# Download and install Spark
RUN wget https://dlcdn.apache.org/spark/spark-3.4.1/spark-3.4.1-bin-hadoop3.tgz && \
    tar -xvzf spark-3.4.1-bin-hadoop3.tgz && \
    mkdir -p /app/opt && \
    mv spark-3.4.1-bin-hadoop3 /app/opt/spark && \
    rm spark-3.4.1-bin-hadoop3.tgz

# Downloading gcloud package
RUN curl https://dl.google.com/dl/cloudsdk/channels/rapid/downloads/google-cloud-cli-442.0.0-linux-x86.tar.gz > /tmp/google-cloud-sdk.tar.gz

# Install gcloud SDK and authenticate
COPY credentials.json credentials.json
RUN tar -C /usr/local -xzf /tmp/google-cloud-sdk.tar.gz && \
    /usr/local/google-cloud-sdk/install.sh && \
    gcloud auth activate-service-account --key-file=credentials.json

# Set environment variables
ENV PYSPARK_PYTHON=python
ENV SPARK_HOME=/app/opt/spark
ENV PATH=$PATH:$SPARK_HOME/bin
ENV GOOGLE_APPLICATION_CREDENTIALS=/app/credentials.json

# Copying requirements and Spark application code into container
COPY requirements.txt spark_app.py ./

# Installing Spark app reqs
RUN pip install -r requirements.txt

# Set the entrypoint to run the Spark job
ENTRYPOINT ["gcloud", "dataproc", "jobs", "submit", "pyspark", \
    "--cluster=pipelineproject01-cluster", \
    "--region=europe-west2", \
    "--jars=gs://spark-lib/bigquery/spark-bigquery-with-dependencies_2.12-0.23.2.jar", \
    "--py-files", "gs://pipelineproject01-code-bucket/code/config_variables.py", \
    "spark_app.py"]